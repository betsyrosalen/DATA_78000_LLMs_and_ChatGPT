# Some thoughts on Bias and Toxicity in Large Language Models

While I think it's a noble and necessary goal to strive to remove bias and toxicity from large laguage models like ChatGPT, I think that ultimately it may prove impossible. At the very least it may be a never-ending iterative process as future generations look at systems created with the biases of our time from a new future perspective that we couldn't possibly anticipate today. That's not to say we shouldn't try and do as much as we can to work toward that goal. I think that we may be able to eliminate the most obvious and egregious forms of bias and toxicity, like hate speech and racism, but if we keep in mind that these are systems created by human beings, embedded within a particular time period, and that they are based on data sets created by human beings, during particular historic or current times, then I don't think that we, as human beings, can reasonably expect to be capable of removing all of our own biases from these systems. Everything we create will be impacted by the standpoint of it's creators. Although we can try to implement data feminism or design justice principles and practices into the development of these systems, there will always be someone whose viewpoint is left out of that process. No matter how much curating we do to those data sets, or how many rules we add to their constitutions, there will always be cases (even if only "edge" cases) where we failed to see our own conscious or unconscious biases. Think for example of the people who may believe that these systems shouldn't even be created in the first place. The fact that many of use see them as inevitable is an outcome of our own technological bias.

Given the possibility that the task may not be feasible, I think we should also be striving to ensure that the way we *use* those systems is as much as possible for the public good. And when I say "the public" I mean people, not corporations, not jobs, and not the economy, but everyday people. Because economic terms are often used to hide corporate agendas that are not benefitting the majority of people nearly as much as they are benefitting the CEOs. I get that people need jobs, at least they do in our current system, but as I will describe later if AI is promising to do away with many of our (menial and less desirable) jobs, then that argument goes way. I would include the environment and animals within that definition too because without a sustainable planet to live on and put food on our plates, then nothing else really matters. This too may be an ultimately unachievable goal, but it certainly won't happen if we don't make a conscious and concerted effort to ensure that things move in that direction. The famous line from Spider-Man comics applies here: "With great power comes great responsibility." Even the tech sector and AI experts agree that the potential power of AI is like nothing we have seen before, so it follows that this kind of power should not be in the hands of power-hungry and money-hungry organizations whether they be corporations or governments. If AI makes good on its techno-utopian promise and takes over the tedious and laborious tasks that most humans don't want to do anyway, then we need to make sure that we have social systems in place to help those whose jobs are automated or eliminated. We also need to make sure that the use of these systems is tightly monitored and regulated so that they cannot be used to psychologically manipulate people (including in some of the ways that we are already seeing them being used) whether it's to buy a product, watch a video, or vote a certain way. I don't have the answers. I don't know how we will be able to get through this time without even more power becoming concentrated in the hands of the corporate and government elite than they already have, but it's a problem that needs to be adressed if we don't want it to go that way.  It's not a problem that we can afford to ignore.