{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "200cde67",
   "metadata": {},
   "source": [
    "## Tokenization lab\n",
    "LLM's and ChatGPT | Fall 2023 | McSweeney | CUNY Graduate Center\n",
    "\n",
    "**Due:** October 8\n",
    "\n",
    "\n",
    "### Background\n",
    "The purpose of this lab is to explore different tokenization methods. On their own, tokenization methods don't do much. However, they are the starting place for all natural language processing. \n",
    "\n",
    "\n",
    "#### Notes\n",
    "This is a short lab using the same dataset throughout. Feel free to switch it up, but once you are comfortable with how the different alogorithms approach the task of breaking up text, move on. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd47efac",
   "metadata": {},
   "source": [
    "You will be using the `datasets` package. You can [install the package](https://pypi.org/project/datasets/) with `$ pip install datasets`. If you do not have `pip` or `conda` installed on your machine, please install it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c7e72fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\cis-a\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import timeit\n",
    "\n",
    "nltk.download('wordnet')  # got an error in cell 10 so added this line and reran...\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420d2c62",
   "metadata": {},
   "source": [
    "The next cell is just downloading the dataset. You need to be connected to the internet for this to work. \n",
    "\n",
    "This dataset is hosted by [Hugging Face](https://huggingface.co). Hugging Face hosts machine learning models, datasets, and more. We will reference them again. It's a great place to find corpora. \n",
    "\n",
    "\n",
    "The dataset is called [American Stories](https://huggingface.co/datasets/dell-research-harvard/AmericanStories). Please skim the Dataset Card. All models and datasets on the Hugging Face hub have these associated cards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a36427c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset american_stories/subset_years to C:/Users/cis-a/.cache/huggingface/datasets/dell-research-harvard___american_stories/subset_years-22ef276adc874771/0.1.0/75a916c5166c4f1fe51a57e0f5074cc72e19157c2bb064a2dc3e6362e19892fb...\n",
      "Only taking a subset of years. Change name to 'all_years' to use all years in the dataset.\n",
      "{'1942': 'https://huggingface.co/datasets/dell-research-harvard/AmericanStories/resolve/main/faro_1942.tar.gz'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1d3e367bbe145868eff39b74f0a3538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0d8eb1039934b4eb239b8f683fc1978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/459M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating 1942 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading associated\n",
      "Dataset american_stories downloaded and prepared to C:/Users/cis-a/.cache/huggingface/datasets/dell-research-harvard___american_stories/subset_years-22ef276adc874771/0.1.0/75a916c5166c4f1fe51a57e0f5074cc72e19157c2bb064a2dc3e6362e19892fb. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80ed37c1a70e40809428b0858f02d87c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The meat ration of the American\n",
      "soldier has been increased about 50\n",
      "per cent to satisfy the soldiers\n",
      "tastes.\n",
      "\n",
      "\n",
      "The coffee ration has been cut\n",
      "almost in half because this genera\n",
      "ton of soldiers drinks milk.\n",
      "\n",
      "\n",
      "These are the outstanding changes\n",
      "which will be incorporated in the\n",
      "1943 edition of the Bible of mess\n",
      "sergeants,\" the Army Cook, on which\n",
      "a group of 10 of the countrys fore\n",
      "most woman nutritionists now are\n",
      "at work.\n",
      "\n",
      "\n",
      "Working under the direction of\n",
      "the Quartermaster Corps, they have\n",
      "undertaken revision of recipes\n",
      "for Army dishes, based on the newer\n",
      "knowledge of nutrition and the\n",
      "changed eating habits in the homes\n",
      "from which the soldiers come.\n",
      "\n",
      "\n",
      "Most of the old standbys of 1918\n",
      "remain. but sometimes with differ\n",
      "ent and more exact proportions.\n",
      "\"Slumgullion\" is practically un-\n",
      "changed Properly cooked, the nu-\n",
      "triton experts say, it is one of the\n",
      "most tasty and satisfying dishes\n",
      "possible. Badly cooked, it may be\n",
      "terrible mess, but it is harder tomake a mess of it than of some\n",
      "other reci\n"
     ]
    }
   ],
   "source": [
    "# Decide what year you want between 1810 and 1963\n",
    "\n",
    "my_year = \"1942\" # the year mom was born\n",
    "\n",
    "# Decide how many articles you want to work with (keep this small - it's slow)\n",
    "num_articles = 15\n",
    "\n",
    "#  Download data for your choice of year (1810 to 1963)\n",
    "dataset = load_dataset(\"dell-research-harvard/AmericanStories\",\n",
    "    \"subset_years\",\n",
    "    year_list=[my_year]\n",
    ")\n",
    "\n",
    "# Get the first n articles from that year\n",
    "# instantiate the counter\n",
    "i=0\n",
    "# instantiate the string\n",
    "my_articles = ''\n",
    "# loop through each article for that year\n",
    "for article in dataset[my_year]:\n",
    "    #the article is a dictionary, \n",
    "    #we're getting the text of the article by accessing the key, \"article\"\n",
    "    my_articles += article.get('article')\n",
    "    #add one to our counter\n",
    "    i+=1\n",
    "    #if the counter is greater than num_articles-1, stop looping\n",
    "    if i>(num_articles-1): break\n",
    "    \n",
    "#validate that it is what we expect by checking on first 100 characters\n",
    "print(my_articles[:1000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935d221a",
   "metadata": {},
   "source": [
    "This section is for formatting. It removes almost all the markup in these articles. It's a fairly standard set of character encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2de5a622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The meat ration of the American soldier has been increased about 50 per cent to satisfy the soldiers tastes.   The coffee ration has been cut almost in half because this genera ton of soldiers drinks milk.   These are the outstanding changes which will be incorporated in the 1943 edition of the Bible of mess sergeants,\" the Army Cook, on which a group of 10 of the countrys fore most woman nutritionists now are at work.   Working under the direction of the Quartermaster Corps, they have undertaken revision of recipes for Army dishes, based on the newer knowledge of nutrition and the changed eating habits in the homes from which the soldiers come.   Most of the old standbys of 1918 remain. but sometimes with differ ent and more exact proportions. \"Slumgullion\" is practically un- changed Properly cooked, the nu- triton experts say, it is one of the most tasty and satisfying dishes possible. Badly cooked, it may be terrible mess, but it is harder tomake a mess of it than of some other reci'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove new line and other formatting characters\n",
    "for char in [\"\\n\", \"\\r\", \"\\d\", \"\\t\"]:\n",
    "    my_articles = my_articles.replace(char, \" \")\n",
    "my_articles[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e522efd",
   "metadata": {},
   "source": [
    "# Whitespace tokenization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55e0c7f",
   "metadata": {},
   "source": [
    "First we'll just break up the words using whitespace. As noted in class, this is a really common first pass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00d1404f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'meat',\n",
       " 'ration',\n",
       " 'of',\n",
       " 'the',\n",
       " 'American',\n",
       " 'soldier',\n",
       " 'has',\n",
       " 'been',\n",
       " 'increased',\n",
       " 'about',\n",
       " '50',\n",
       " 'per',\n",
       " 'cent',\n",
       " 'to',\n",
       " 'satisfy',\n",
       " 'the',\n",
       " 'soldiers',\n",
       " 'tastes.',\n",
       " '']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#this is a magic function to determine how long a cell takes to run. \n",
    "#It MUST be the first thing in a cell\n",
    "\n",
    "#split the whole string on spaces. This returns a list\n",
    "whitespace_tokens = my_articles.split(' ')\n",
    "\n",
    "#check the list\n",
    "whitespace_tokens[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb27496",
   "metadata": {},
   "source": [
    "Note: \"µs\" is microseconds, or a millionth of a second 1/1,000,000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4bf8ea",
   "metadata": {},
   "source": [
    "# Morphological Tokenization \n",
    "\n",
    "Lemmatizing is the process of breaking down text into tokens by first breaking it up into \"words\" and then using syntactic knowledge of the language (in this case, English) to break up the words. \n",
    "\n",
    "Princeton maintains the [morphy project](https://wordnet.princeton.edu/documentation/morphy7wn#:~:text=Morphology%20in%20WordNet%20uses%20two,word%20that%20is%20in%20WordNet.), which powers `nltk`'s [WordNet Lemmatizer](https://www.nltk.org/api/nltk.stem.wordnet.html). You do NOT need to read this entire documentation, just acknowledge that it requires a significant amount of knowledge about English in order to make it work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "108e29a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This lemmatizer is based on the Morphy project above\n",
    "from nltk.stem import WordNetLemmatizer\n",
    " \n",
    "#Uncomment these two lines - you may need to download these, maybe not. \n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')\n",
    "wn_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16560507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.88 s\n",
      "Wall time: 1.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'meat',\n",
       " 'ration',\n",
       " 'of',\n",
       " 'the',\n",
       " 'American',\n",
       " 'soldier',\n",
       " 'ha',\n",
       " 'been',\n",
       " 'increased',\n",
       " 'about',\n",
       " '50',\n",
       " 'per',\n",
       " 'cent',\n",
       " 'to',\n",
       " 'satisfy',\n",
       " 'the',\n",
       " 'soldier',\n",
       " 'tastes.',\n",
       " '']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#first we have to split the string on spaces to get \"words\"\n",
    "whitespace_tokens = my_articles.split(' ')\n",
    "\n",
    "my_lemmas = []\n",
    "for word in whitespace_tokens:\n",
    "    w = wn_lemmatizer.lemmatize(word)\n",
    "    my_lemmas.append(w)\n",
    "my_lemmas[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883aa4ba",
   "metadata": {},
   "source": [
    "Notice how much time it takes to tokenize on whitespace versus using morphological rules. Also notice if it produced the output you expected. Sometimes it doesn't. \n",
    "\n",
    "ms is a millisecond, or one one thousandth of a second 1/1,000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d79061",
   "metadata": {},
   "source": [
    "# Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962f282d",
   "metadata": {},
   "source": [
    "There are two implementations of BPE here. The first [uses a package (bpe)](https://github.com/soaxelbrooke/python-bpe) that you will have to install using `pip` (see above). \n",
    "\n",
    "This will implement the algorithm we covered in class and that you can review at [Hugging Face](https://youtu.be/HEikzVL-lZU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f1639f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bpe import Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8857be2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 31.2 ms\n",
      "Wall time: 26.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "whitespace_tokens = my_articles.split(' ')\n",
    "\n",
    "# calling the Encoder algorithm\n",
    "# we've specified 100 token vocab and 95% to be tokenized\n",
    "# the other 5% is transformed into UNK\n",
    "encoder = Encoder(100, pct_bpe=0.95)\n",
    "encoder.fit(whitespace_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae30d518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the meat ration of the american soldier has been increased about __unk0 per cent to satisfy the soldiers tastes . the coffee ration has been cut almost in half because this genera ton of soldiers drinks milk . these are the outstanding changes which will be incorporated in the 1__unk__unk__unk edition of the bible of mess sergeants __unk__unk the army cook , on which a group of 10 of the countrys fore most woman nutritionists now are at work . working under the direction of the __unkuartermaster corps , they have undertaken revision of recipes for army dishes , based on the newer knowledge of nutrition and the changed eating habits in the homes from which the soldiers come . most of the old standbys of 1__unk1__unk remain . but sometimes with differ ent and more e__unkact proportions . __unk slumgullion __unk is practically un - changed properly cooked , the nu - triton e__unkperts say , it is one of the most tasty and satisfying dishes possible . badly cooked , it may be terrible mess , but it is harder tomake a mess of it than of some other recipes . baked beans have __unkust as much prominence as ever and there seldom is any ob__unkection to them . gold fish __unk eliminated . the one fundamental dish which has been all but eliminated is gold fish __unk because it is becoming rapidly __unkust what the name signifies canned salmon , which in the last war sometimes was served three times a day , is now a dish for wealthy epicures . the army cook followed by mess sergeants of the first world war re - manned essentially the same as in 1__unk__unk0 . it was . not actually revised until 1__unk__unk__unk when the new science of nutrition came over the hori__unkon since then it has kept abreast of scientific advances . when the present army began to e__unkpand with selectees 1t soon was reali__unked that the men were reflect ing the changed eating habits of the american people . they came from a milk - drinking age . 1t used to re__unkuire seven pounds of coffe per 100 men per meal . this made full allowance for seconds __unk now , 1thowever . hitler __unk s soldiers must de - pend largely on concoctions of soy beans . marines once got rum . the revisers of the army cook are not worrying over minerals and vitamins . if soldier eats his full ration he is bound to get enough of all of them . 1t has developed . however . that the new soldier is not much of vegetable eater . before long , it is hoped . he will develope tastes not cultivated at home . desserts are prescribed at least twice day and scores of tasty recipes are being prepared or re - vised . the oldest ration sheet the other day on the desk of miss mary barber in charge of the revision vas one issued for american marines on shipboard in 1__unk__unk__unk . for si__unk men per week it called for a__unk pounds of bread , il pounds of flour , il pounds of beef , ii pounds of pork . a__unk pounds of cheese . ii pints of dried peas , l__unk pints of oatmeal and pints of vinegar . 1t also called for __unkl__unk pounds of butter , supposedly including most other fats . and a__unk pints of rum , or pint per man per day . improving __unkuality . the present work of revision , 1t is e__unkplained , is not so much directed at developing new dishes as at pre - scribing more precisely amounts and __unkuality of ingredients a pinch of salt in the right place might mean tasty dish when two pinches of salt . carelessly put into a pot by an uninstructed cook , might ruin the __unk flavor . after all , miss barber e__unkplains , there is little need to change the __unk fundamental dishes . they are the result of long e__unkperience in the tastes of soldiers and the average recruit soon develops an appetite for them , if they are properly pre - pared . the nations food industries have provided their best e__unkperts for the work . a total of __unk , l__unk1 students are reg istered in the second si__unk - week term of the george washington univer - sity summer sessions . registrar fred everett nessell announced yes terday . this is the first year that the university has operated on a two term basis in the summer . the registration for the current session like that of the first si__unk - week term has broken all previous summer enrolment records . there were __unk , __unk0a registered , plus __unk00 engaged in special war training work with tut ton paid by the government , in the session that ended two weeks ago , all of the __unk__unk states , 10 foreign countries and the district of co lumbia are represented in the sum - mer registration . the district leads with 1__unk__unk__unk students enrolled , new york is second with 1__unk__unk__unk , virginia third with 1__unk__unk , maryland fourth with lo__unk and pennsylvania fifth with __unk0 . is found , lots of the men drink no coffee and few return for a second cup . the allowance has been cut down to four pounds . a half pint of milk , when it can be obtained , is allowed the soldier for breakfast , this has raised an acute problem in some southern camps in areas where there are few dairies . bigger meat eaters . the new soldiers are bigger meat eaters . the allowance now is so pounds of carcass beef per 100 men per meal , instead of the old ration of as pounds . this includes bones , fat and all . comparative substitute allowances are made when boned beef is served in world war days the total ra - ton provided __unk , __unk00 calories per day . the heat - energy produced by food is measured in calories - they mostly come from starches , fats and sugars . 1t is estimated that an active civil ian needs __unk , __unk00 a day . in the new army cookbook . however . this is cut down to 1__unk00 . 1t was found that the average soldier would not eat enough , in spite of sharpened ape tite , to supply the allegedly ideal amount and presumably did not need it . on the other hand the amount of protein has been almost precisely doubled over that prescribed as ideal for civilians . proteins are provided largely by lean meats . information reaching here is that the german ration recogni__unkes the same necessity and provides almost e__unkactly the same amount of protein as the american . instead of __unkuicy meats , if you sre 1s to __unk__unk and physically it , you may apply for enlistment direct in the signal corps or in the signal corps enlisted reserve . direct eaiisfmenf __unk e__unkperience licensed radio operator , a trained radio reparman , a telephone telegraph worker , will __unkualify you for active duty at once . from private __unk s pay you can advance rap idly a . you higher technical ratngs - up to __unk 1__unk__unk a month , with board , shelter and uniforms . esil . tsd reserve __unk if you are skilled with tools but lack __unkualifying e__unkperience , you may the enlisted reserve . you will be given training , with pay in of the many signal corps schools , and ordered to active duty when you have completed the course . commissions graduate electrical engineers may apply for immediate commissions in the signal corps . and are open __unkuniors and seniors in electrical engi . nearing colleges __unk- four persons were in__unkured , two of them seriously enough to re__unkuire dis__unkualification . in traffic accidents late yesterday , police reported . miss __unkewel n__unk hampton , __unk0 , of 1__unk__unk1 eighteenth street nv , stepped from a safety __unkone in the __unk__unk00 block , of eighteenth street nv into the path of ta__unki driven by george a . snickerson , __unk1 , of __unk0 evarts street e , police said . she was treated at emergency hospital for head in , iuries . her condition was unde - , termined at late hour last night . at the intersection of fifth street fsnd concord avenue nv . an auto driven by thomas a . wilson , __unk__unk of __unk__unk__unk1 1angdrum lane , chevy chase , md , according to police , collided with capital transit bus . in__unkuring mr __unk__unk wilson __unk s mother mrs . esther a . wilson . as , who was riding in the car with her son . b mrs . wilson was admitted to casualty hospital with head lac erations , bruises and a back in __unkury . driver of the bus was __unkohn s . roberts , __unk1 , of __unk010 powhatan road , shyattsville , md , police reported . __unk at no . precinct police charged mr __unk__unk wilson with failing to yield the right of way . __unk william t . cooper , __unk . of __unk__unk10 new hampshire avenue nv , was struck by an automobile driven by oral o . husk . 10 , of __unk__unk__unk massachusetts , ave - nue . nv , in an accident on seventh street between l . and m streets n w __unk__unk police reported . the child , treated at emergency hospital for minor in__unkuries , was walking with his brother , robert , ii , when he broke away from the older boy and ran into the street , police said . leonard stevens , 10 , colored , of 1__unk__unk__unk b street n e . roller skated from between parked cars in front of 1__unk__unk0 b street ne , according to police , and was struck by an auto driven by o . b . petty , 1__unk__unk__unk linden street ne , police records show . the child was treated at casualty hos pital for scratches and bruises and releasedthe montgomery county __unk mdl policemen __unk s baseball team yesterday defeated a team composed of county firemen . 1__unk11 , in their annual game at the colesville diamond police man __unkames anderson . stationed at silver spring , pitched for the police men . william hall , a member of the bethesda fire department __unk__unk and dr __unk__unk paul everett , bethesda au__unkiliary fireman . pitched for the losers . another landmark in washington must go b __unk n once again the heehineer , company has contracted to demoiish washington land mark --- the national h0tel --- at one time __unk washineton __unk s finest __unk and we __unk ll re - nembered as stopping place of presidents and statesmen . as washington e__unkpands and grows many such landmarks have to give way before the es progress we are proud of the part have played in making this more beautiful city , for __unk chaneng washngtons skyline has been our work for __unk1 years __unk__unk the material is for snl__unkl as this hotel demolished the material will be hauled 10 four yards and reeondioned no sales will be made at the __unkob ss hechnger offer you treew saving of time effort sad money our yards have complete stock both and ssed building materials all neatly arranged give you wide selection prompt courteous service . resort hotels in fire may revive stage coach service to large cities . ii you are ever is year , of age , and even though registered for selective service , have not received your order to report for undue . ton , the signal corps offers you an out . standing opportunity . if you have ability with tools if you want to secure training in the vitality i __unk m . portant field of communications may attend __unk school in or near your home city . you will be paid not less than __unk000 per year while learning . and when you have finished your training - in months or less you can advance to higher pay - your technical skill increases . even if you have __unk minor physical handi . cap , signal corps civilian training may give you the chance you __unk ve wanted to ssrvs the army of the united seataa . curb fifth column and espionage activities . interest in the move here was in creased by reports from me__unkico that citi__unkenship papers of all nationals of a__unkis or a__unkis dominated coun - tries there would be cancelled where fraud was shown or where the new citi__unkens failed to meet their obliga - tions of citi__unkenship . under the cuban regulation , an nounced yesterday by the ministry of state , no cuban consular or dip lomatic agents in europe will be au - thori__unked to issue visas for entry into cuba without specific authority of the ministry of state . this will give the central govern ment opportunity to revise every such application and 1t was taken as foregone conclusion that re - __unkuests for visas would be denied al most automatically , e__unkcept in e__unk - traordinary cases . ry the associated press . havana . aug . __unk__unk __unk- cuba put in force today an order to halt all immigration from europe , and to suspend all citi__unkenship petitions now under consideration in a move toa standard course of first - aid in struction is being given at the mount rainier elementary school , thirty second street and bunker hill road , those interested in __unkoin ing the class are asked to attend the ne__unkt regular meeting at __unk __unk__unk __unko pm . wednesday . andrew r . gill , __unkr __unk__unk and frank s . taylor are in charge of instruction . bra__unkil will establish a new bank for financing rubber producers .\n"
     ]
    }
   ],
   "source": [
    "#print(encoder.tokenize(my_articles))\n",
    "\n",
    "print(next(encoder.inverse_transform(encoder.transform([my_articles]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90324f0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13beaf38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
