# Lab 2 - N-grams-Tokenization-WordVectors

I replaced the Great Gatsby with The War of the Worlds and ran through all the code using that text first to make sure everything ran ok and that I understood what it was doing.  I tweaked a few things here and there to make it run and get a better understanding but for the most part it ran pretty smoothly.

Next I switched the text to Alice in Wonderland and ran through all the same code to create a bigram model and generate text from that.  The output was interesting from both of the first two models because althoug it was basically gibberish you could still tell the difference in style between the two writers.  

Finally, I tried (just for fun) to do a trigram model also using Alice in Wonderland, which I think came out pretty good and I think I did everything correctly.  The output seemed to be a little better (in terms of making sense) than the bigram model, but not by much.  